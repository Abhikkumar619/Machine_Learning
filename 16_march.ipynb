{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b560ff85-a019-41c9-92b3-3d270938133e",
   "metadata": {},
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23774793-c2d1-4c7f-94c1-834d22e9d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Overfitting : When the model fits the traning data too well and perfome poorly in new data\n",
    "              called overfitting.\n",
    "              \n",
    "consequences\n",
    "It occurs when ML model is too complex and tries to fix the noise or random flucuations in the training\n",
    "data. as a result it perfome very well in training data but poorly on new or unseen data.\n",
    "\n",
    "Mitigated\n",
    "- Increase training data, more data can help the model capture a broader range of pattern, \n",
    "  reduce the chances of overfitting.\n",
    "-feature selection : Remove the irrelevent or noisy features which might confuses the model.\n",
    "\n",
    "\n",
    "Underfitting: When the model fits the training data poorly and perfome well on new data\n",
    "              callled underfitting.\n",
    "consequences\n",
    "It occurs when ML is too simple and cannot capture the underlaying patterns in the traning\n",
    "data as a result dit perfome poorly on the training data also in neq or unseen data.\n",
    "\n",
    "mitigated.\n",
    "\n",
    "1.Feature_engineering: create more relevent features that can better represent the underlying patterns in the data.\n",
    "\n",
    "2. Increase the model_complexity: use more advance or flexible model that has a higher capacity to \n",
    "                                 learn complex relationships.\n",
    "3. add more features: if possible, include additional relevent features that can help the model capture the underlaying patterns.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c03b792-4804-431f-8cee-18386e131a71",
   "metadata": {},
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f7883-0610-4bc9-b26e-2e5d2f991aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we can reduce overfitting by increasing training data an removing irrelevent features from dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511f84ef-f2d1-4e3e-9e16-1edb2ceba6da",
   "metadata": {},
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced1cf0c-7d62-4800-8a91-6a651111775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "uderfitting occurs when a model is to simple or lacks the capacity to capture the unerlying patterns in the data.\n",
    "it fails to learn the essential realtionships an exhibits high bias.\n",
    "\n",
    "scenarios of underfitting in ML \n",
    "1. INsuffiecient model complexity : if the chosen model is too simple an leaks the capacity to capture the underlying patterns in the data.\n",
    "\n",
    "2. Limited training data : when tha available traing data is insufficient to capture the complexity of the problem, the model may struggle to generalize well result underfitting.\n",
    "\n",
    "3. inadequate feature representation : if the features use to train the model o not adequately represent the unerlaying patterns in the data, the model may inderfit.\n",
    "\n",
    "4. high regularization strangth : Regularzation technique like L1 or L2 regularization strength is set to high. it can overly constrain the model and lead to underfitting. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e96b6-bd5f-4996-aa03-f04e5c4ef347",
   "metadata": {},
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c0bc0-b617-499f-a689-0692ed780821",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The bias-variance tradeoff is a fundamental concept in machine learning that describes\n",
    "the relationship between the complexity of a model and its ability to generalize to new \n",
    "data. Bias refers to the error that is introduced by approximating a real-world problem \n",
    "with a simplified model. High bias models are typically too simple and may not capture \n",
    "the underlying patterns in the data. Variance refers to the error that is introduced by\n",
    "modeling the random noise in the training data. High variance models are typically too \n",
    "complex and may overfit the training data, leading to poor generalization performance \n",
    "on new data. The goal of machine learning is to find a balance between bias and variancethat minimizes\n",
    "the total error of the model on new data. This is known as the bias-variance tradeoff\n",
    "\n",
    "Relationship between bias and variance. \n",
    "High bias and low variance: A model with high bias tends to have a simpler structure and\n",
    "makes strong assumptions about the data. It may underfit the training data and have\n",
    "limited flexibility to capture complex relationships. However, the\n",
    "predictions of such a model are relatively consistent across different training sets.\n",
    "\n",
    "Low bias and high variance: A model with low bias has high flexibility and can capture\n",
    "complex patterns in the training data. However, it is prone to overfitting and performs\n",
    "poorly on new data because it captures noise and idiosyncrasies specific to the training\n",
    "set. Consequently, \n",
    "the predictions of this model can vary significantly across different training sets\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bb61a9-31fa-4d19-8481-3d6c46b78758",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in\n",
    "machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15949d33-baf8-4e69-88f6-a9ab73cc797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation Metrics:\n",
    "Compare the model's performance on the training set and a separate holdout validation \n",
    "or test set using appropriate evaluation metrics. If the model has significantly higher\n",
    "performance on the training set than the validation/test set, it may be overfitting.\n",
    "Conversely, if both \n",
    "the training and validation/test performance are poor, it may indicate underfitting.\n",
    "\n",
    "Cross-Validation: Use techniques like k-fold cross-validation to assess the model's \n",
    "performance on multiple subsets of the data. If the model consistently performs well\n",
    "across different folds, it suggests good generalization. However, \n",
    "if the model's performance \n",
    "varies significantly across folds, it may indicate overfitting or underfitting.\n",
    "\n",
    "Regularization Effects:\n",
    "If you apply regularization techniques like L1 or L2\n",
    "regularization, examine the effect on the model's performance.\n",
    "Increasing the regularization strength\n",
    "may help mitigate overfitting, while decreasing it could alleviate underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433967e1-147e-4f54-8321-6a1b067f3759",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c107c46a-782b-4453-83bd-00e30c7566dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bias:\n",
    "\n",
    "1.Bias represents the error introduced by approximating a real-world problem with\n",
    "a simplified model.\n",
    "2.High bias models have a tendency to oversimplify the problem, making strong\n",
    "assumptions and ignoring complex patterns in the data.\n",
    "3.These models are typically too rigid and inflexible, leading to underfitting,\n",
    "where they fail to capture the true underlying relationships in the data.\n",
    "4.High bias models have low complexity and exhibit systematic errors, resulting in \n",
    "consistent but inaccurate predictions.\n",
    "   Examples of high bias models include linear regression with insufficient features to capture nonlinear relationships, or decision trees with limited depth that cannot capture complex decision boundaries.\n",
    "\n",
    "\n",
    "Variance:\n",
    "Variance represents the variability of the model's predictions for different training\n",
    "sets.\n",
    "1.High variance models are overly complex and highly flexible, capable of capturing \n",
    "noise and idiosyncrasies of the training data.\n",
    "2.These models tend to overfit the training data, memorizing its peculiarities and \n",
    "struggling to generalize to new, unseen data.\n",
    "3.High variance models have high complexity and exhibit high sensitivity to small\n",
    "fluctuations in the training data, resulting in inconsistent and unstable predictions.\n",
    "4.Examples of high variance models include deep neural networks with many layers, \n",
    "decision trees with large depths, or models with a high number of parameters relative \n",
    "to the available data.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efa95fd-221a-4363-96c2-66e51652bde3",
   "metadata": {},
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381370c5-b7f7-416d-b512-47cbfa7e82e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Regularization is a technique used in machine learning to prevent overfitting by adding a penalty to the model's loss function. It introduces a bias to the learning process,\n",
    "discouraging the model from becoming too complex and overly specialized to the training \n",
    "data. Regularization helps to achieve a balance between model simplicity and its ability\n",
    "to capture the underlying patterns in the data.\n",
    "\n",
    "Here are some common regularization techniques and how they work:\n",
    "\n",
    "L1 Regularization (Lasso Regression):\n",
    "\n",
    "L1 regularization adds the sum of the absolute values of the model's coefficients\n",
    "to the loss function.\n",
    "It encourages sparsity by pushing the coefficients of irrelevant features to zero, \n",
    "effectively performing feature selection.\n",
    "L1 regularization is effective when there are many irrelevant or redundant features,\n",
    "as it shrinks their impact on the model.\n",
    "\n",
    "L2 Regularization (Ridge Regression):\n",
    "\n",
    "L2 regularization adds the sum of the squared values of the model's coefficients to\n",
    "the loss function.\n",
    "It discourages large coefficient values, as larger coefficients contribute more to \n",
    "the loss.\n",
    "L2 regularization is effective in reducing the impact of individual features without\n",
    "necessarily eliminating them entirely.\n",
    "\n",
    "Elastic Net Regularization:\n",
    "\n",
    "Elastic Net regularization combines L1 and L2 regularization, \n",
    "incorporating both the absolute and squared values of the coefficients into the \n",
    "loss function.\n",
    "It provides a balance between L1 and L2 regularization, combining their benefits\n",
    "in feature selection and coefficient shrinkage.\n",
    "Elastic Net regularization is useful when there are many features with high collinearity.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
